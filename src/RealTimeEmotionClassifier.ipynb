{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RealTimeEmotionClassifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suYrlchEd1f9",
        "colab_type": "text"
      },
      "source": [
        "Importing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQhHzQX0eJ81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Flatten, Dense, Dropout\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.optimizers import SGD\n",
        "import cv2"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umVRqY9weOK7",
        "colab_type": "text"
      },
      "source": [
        "Importing the dataset - fer2013"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVKev-N6egwz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "aabc1d2d-ccdc-4c1e-84ee-86b2e94486c5"
      },
      "source": [
        "emotion_data = pd.read_csv('/content/fer2013.csv')\n",
        "print(emotion_data)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       emotion                                             pixels        Usage\n",
            "0            0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...     Training\n",
            "1            0  151 150 147 155 148 133 111 140 170 174 182 15...     Training\n",
            "2            2  231 212 156 164 174 138 161 173 182 200 106 38...     Training\n",
            "3            4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...     Training\n",
            "4            6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...     Training\n",
            "...        ...                                                ...          ...\n",
            "35882        6  50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...  PrivateTest\n",
            "35883        3  178 174 172 173 181 188 191 194 196 199 200 20...  PrivateTest\n",
            "35884        0  17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...  PrivateTest\n",
            "35885        3  30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...  PrivateTest\n",
            "35886        2  19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...  PrivateTest\n",
            "\n",
            "[35887 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr9n_7I1kRzj",
        "colab_type": "text"
      },
      "source": [
        "fer2013 is an open-source data set which contains 48 X 48-pixel grayscale images of the face. There are seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral) present in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbpuyfRjgBdj",
        "colab_type": "text"
      },
      "source": [
        "Separating testing and training data and reshaping them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4gUHuE1gK2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = []\n",
        "y_train = []\n",
        "X_test = []\n",
        "y_test = []\n",
        "t = 0\n",
        "for index, row in emotion_data.iterrows():\n",
        "    k = [int(i) for i in row['pixels'].split(\" \")]\n",
        "    if row['Usage'] == 'Training':\n",
        "        X_train.append(np.array(k))\n",
        "        y_train.append(row['emotion'])\n",
        "    elif row['Usage'] == 'PublicTest':\n",
        "        X_test.append(np.array(k))\n",
        "        y_test.append(row['emotion'])\n",
        "\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)\n",
        "\n",
        "\n",
        "y_train= np_utils.to_categorical(y_train, num_classes=7)\n",
        "y_test = np_utils.to_categorical(y_test, num_classes=7)\n"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atB_nB65gkXV",
        "colab_type": "text"
      },
      "source": [
        "Building the model for emotion detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33jeNZrjgowG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(ZeroPadding2D((1,1),input_shape=(48,48,1)))\n",
        "model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2), padding='same'))\n",
        "\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2), padding='same'))\n",
        "\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2), padding='same'))\n",
        "\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2), padding='same'))\n",
        "\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2), padding='same'))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(4096, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(4096, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='sgd',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMK9mPUZhlI0",
        "colab_type": "text"
      },
      "source": [
        "Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNnH3PErhuYZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0b6ab769-0c7a-4da7-b740-3a6d4a0a43af"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "zero_padding2d_75 (ZeroPaddi (None, 50, 50, 1)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_84 (Conv2D)           (None, 16, 16, 64)        640       \n",
            "_________________________________________________________________\n",
            "zero_padding2d_76 (ZeroPaddi (None, 18, 18, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_85 (Conv2D)           (None, 6, 6, 64)          36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_34 (MaxPooling (None, 3, 3, 64)          0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_77 (ZeroPaddi (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_86 (Conv2D)           (None, 1, 1, 128)         73856     \n",
            "_________________________________________________________________\n",
            "zero_padding2d_78 (ZeroPaddi (None, 3, 3, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_87 (Conv2D)           (None, 1, 1, 128)         147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_35 (MaxPooling (None, 1, 1, 128)         0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_79 (ZeroPaddi (None, 3, 3, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_88 (Conv2D)           (None, 1, 1, 256)         295168    \n",
            "_________________________________________________________________\n",
            "zero_padding2d_80 (ZeroPaddi (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_89 (Conv2D)           (None, 1, 1, 256)         590080    \n",
            "_________________________________________________________________\n",
            "zero_padding2d_81 (ZeroPaddi (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_90 (Conv2D)           (None, 1, 1, 256)         590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_36 (MaxPooling (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_82 (ZeroPaddi (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_91 (Conv2D)           (None, 1, 1, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "zero_padding2d_83 (ZeroPaddi (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_92 (Conv2D)           (None, 1, 1, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "zero_padding2d_84 (ZeroPaddi (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_93 (Conv2D)           (None, 1, 1, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_37 (MaxPooling (None, 1, 1, 512)         0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_85 (ZeroPaddi (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_94 (Conv2D)           (None, 1, 1, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "zero_padding2d_86 (ZeroPaddi (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_95 (Conv2D)           (None, 1, 1, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "zero_padding2d_87 (ZeroPaddi (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_96 (Conv2D)           (None, 1, 1, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_38 (MaxPooling (None, 1, 1, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 4096)              2101248   \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 7)                 28679     \n",
            "=================================================================\n",
            "Total params: 33,624,775\n",
            "Trainable params: 33,624,775\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpjeirfDh-ZM",
        "colab_type": "text"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1E2K--OiShP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(X_train,y_train, batch_size=32, epochs=30, verbose=1, validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RoH5Qbdik55",
        "colab_type": "text"
      },
      "source": [
        "Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTOxw2AXisC5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "60c311e7-c994-4017-951f-b9188e31313d"
      },
      "source": [
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "  json_file.write(model_json)\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24Oif3X9eDou",
        "colab_type": "text"
      },
      "source": [
        "Real Time Emotion Classification using OpenCV and WebCam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uophK8bi_Il",
        "colab_type": "text"
      },
      "source": [
        "import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIM3Ex3wjC_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from keras.models import model_from_json\n",
        "from keras.preprocessing import image"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smrvTvQSjQXa",
        "colab_type": "text"
      },
      "source": [
        "Loading the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLghk2xqjbbe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model_from_json(open(\"model.json\", \"r\").read())\n",
        "model.load_weights('model.h5')"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvANERz_jh5k",
        "colab_type": "text"
      },
      "source": [
        "Detecting Faces and classifying Emotions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5ZLcuDsjx5q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "fdbbfbca-65bc-40a7-ac52-b6f3ecc00fc0"
      },
      "source": [
        "face_haar_cascade = cv2.CascadeClassifier(\n",
        "    'haarcascade_frontalface_default.xml')\n",
        "\n",
        "cap=cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    test_image=cap.read()\n",
        "\n",
        "    coverted_image= cv2.cvtColor(test_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "\n",
        "    faces_detected = face_haar_cascade.detectMultiScale(converted_image)\n",
        "    for (x,y,w,h) in faces_detected:\n",
        "      cv2.rectangle(test_image,(x,y), (x+w,y+h), (255,0,0))\n",
        "      roi_gray=gray_image[y:y+w,x:x+h]\n",
        "      roi_gray=cv2.resize(roi_gray,(48,48))\n",
        "      image_pixels = image.image_to_array(roi_gray)\n",
        "      image_pixels = np.expand_dims(image_pixels, axis = 0)\n",
        "      image_pixels /= 255\n",
        "\n",
        "\n",
        "    predictions = model.predict(image_pixels)\n",
        "    max_index = np.argmax(predictions[0])\n",
        "\n",
        "    emotion_detection = ('angry', 'disgust', 'fear', \n",
        "                         'happy', 'sad', 'surprise', 'neutral')\n",
        "    emotion_prediction = emotion_detection[max_index]\n",
        "\n",
        "\n",
        "    cv2.putText(test_image, emotion_prediction, (int(x), int(y)))\n",
        "    cv2.putText(frame,label,label_position,cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)  \n",
        "\n",
        "\n",
        "    resize_image = cv2.resize(image, (1000, 700))\n",
        "    cv2.imshow('Emotion',resized_image)\n",
        "    if cv2.waitKey(10) == ord('b'):\n",
        "        break\n",
        "\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-cdaf60d657b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtest_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcoverted_image\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Expected Ptr<cv::UMat> for argument '%s'"
          ]
        }
      ]
    }
  ]
}